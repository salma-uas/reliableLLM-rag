{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-augmented generation(RAG) using Open-Source tools\n",
    "\n",
    "### Overview of the Project\n",
    "\n",
    "This project demonstrates an implementation of Retrieval-Augmented Generation (RAG) using an open-source Large Language Model (LLM)-llama3.2 and Pinecone as a vector database.\n",
    "\n",
    "RAG enhances LLMs by fetching real-time data from your documents before generating answers. It ensures responses come directly from your trusted documents—not guesses—making it essential for domains where accuracy and privacy matter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents collection and Library installation\n",
    "\n",
    "Create a folder named `docs`in the same directory as your notebook. Add your files to `./docs/` (supports: `.pdf`, `.docx`, `.txt`).\n",
    "\n",
    "**Example files to use:**  \n",
    "- University course syllabi (PDF)   \n",
    "- Research papers (PDF)\n",
    "- Company policy documents (Word/PDF) \n",
    "- Product manuals (PDF) \n",
    "\n",
    "*Required*: Add at least 2-3 documents for the RAG pipeline to work.\n",
    "\n",
    "### Install Dependencies  \n",
    "Run the cell below to install required dependencies\n",
    "\n",
    "**Key libraries:**  \n",
    "- `llama2` (via Ollama) - Local LLM  \n",
    "- `sentence-transformers` \n",
    "- `pinecone-client` \n",
    "These pachages are needed for Document parsing (PDF, DOCX), Text chunking and embedding, Vector database (Pinecone) and LLM integration via LangChain and Ollama\n",
    "\n",
    "*Note*: Requires Python ≥3.8 and pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pinecone-client langchain llama-cpp-python sentence-transformers chromadb\n",
    "pip install pymupdf python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Documents (TXT, PDF, Word)\n",
    "Read documents from a folder (`docs/`) and prepare them for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import docx\n",
    "\n",
    "# Function to extract text from different document formats\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        text = ''\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            for page in pdf:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "    else:\n",
    "        return None  # Unsupported format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from the 'docs/' folder\n",
    "def load_documents(folder_path='docs/'):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        text = extract_text_from_file(file_path)\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking the documents\n",
    "used a chunk size of 500 and chunk overlap of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13663 text chunks.\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunks = text_splitter.split_text(' '.join(docs))\n",
    "\n",
    "print(f'Loaded {len(chunks)} text chunks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed chunks and Store in Pinecone\n",
    "we use `sentence-transformers` to convert each chunks into vector embeddings and store them in Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "with open(\"Pinecone_API_key.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    API_KEY = file.read()\n",
    "\n",
    "# Initialize Pinecone (Replace 'YOUR_API_KEY' with your Pinecone API key)\n",
    "pc = Pinecone(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag-pipeline\"\n",
    "dimension = 384  # Dimension of the model we're using (e.g., all-MiniLM-L6-v2)\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model: to map sentences / text to embeddings.\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector records for the chunks\n",
    "vectors = [\n",
    "    {\n",
    "        \"id\": str(i),\n",
    "        \"values\": embedder.encode(chunk).tolist(),\n",
    "        \"metadata\": {\"text\": chunk}\n",
    "    }\n",
    "    for i, chunk in enumerate(chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 13663 vectors into 'rag-tutorial'.\n"
     ]
    }
   ],
   "source": [
    "# Upsert = Update values and metadata if exist, Insert if not. This allows safe re-run of code without creating duplicates.\n",
    "# Upsert in small batches to avoid payload limit\n",
    "batch_size = 50\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    batch = vectors[i:i + batch_size]\n",
    "    index.upsert(batch)\n",
    "\n",
    "print(f\"✅   Inserted {len(vectors)} vectors into '{index_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an Open-Source LLM and embedd your question\n",
    "We use Open-Source LLM(Ollama) locally for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "# Creates an instance of the LLM interface, pointing to a local Ollama model called \"llama3.2\".  \n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "question = \"Who is the professor of the course Data Science ?\" # Your Question\n",
    "# Embed question\n",
    "question_vector = embedder.encode(question).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Relevant Information from pinecone db and Prompt Engineering**\n",
    "We retrieve top-10 relevant documents and use them for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Pinecone\n",
    "results = index.query(\n",
    "    vector=question_vector,\n",
    "    top_k=10,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "#  Prepare retrieved context\n",
    "contexts = [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
    "context_text = \"\\n\\n\".join(contexts)\n",
    "\n",
    "# Build prompt\n",
    "prompt = f\"\"\"Answer the question based only on the context below.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Query local LLM via Ollama to get an answer\n",
    "response = model.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
